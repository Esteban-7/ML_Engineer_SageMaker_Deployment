{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dc8c20df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The sagemakar package that was prevously installed is imported. \n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "#The current session information is saved so it can be accessed later. \n",
    "session = sagemaker.Session() # Store the current SageMaker session\n",
    "role = get_execution_role()\n",
    "prefix = 'crossSell-xgboost_2' # a prefix is set so later on a folder on s3 can carry said name\n",
    "\n",
    "test_location = session.upload_data(\"processed_data_2/test.csv\", key_prefix=prefix)\n",
    "val_location = session.upload_data(\"processed_data_2/validation.csv\", key_prefix=prefix)\n",
    "train_location = session.upload_data(\"processed_data_2/train.csv\", key_prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "29c915d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "There is a more up to date SageMaker XGBoost image. To use the newer image, please set 'repo_version'='1.0-1'. For example:\n",
      "\tget_image_uri(region, 'xgboost', '1.0-1').\n"
     ]
    }
   ],
   "source": [
    "#the sagemaker image for the xgboost estimator is now imported to create the container in which \n",
    "#the algorithm will run\n",
    "\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "container = get_image_uri(session.boto_region_name, 'xgboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0cc809c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "#The estimator is now set, the information about the sagemaker session, container, role, etc.\n",
    "#is passed as parameters.\n",
    "xgb = sagemaker.estimator.Estimator(container, # The location of the container we wish to use\n",
    "                                    role,                                    # What is our current IAM Role\n",
    "                                    train_instance_count=1,                  # How many compute instances\n",
    "                                    train_instance_type='ml.m4.xlarge',      # What kind of compute instances\n",
    "                                    output_path='s3://{}/{}/output'.format(session.default_bucket(), prefix),\n",
    "                                    sagemaker_session=session)\n",
    "\n",
    "#The hyperparameters for the estimator are set. It is important to know that these are the baseline parameters\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        early_stopping_rounds=10,\n",
    "                        num_round=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "838523dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#With the estimator created and the baseline hyperparameters set. \n",
    "#An hyperparameter tuner is created and the estimator is passed as parameter.\n",
    "\n",
    "from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "xgb_hyperparameter_tuner = HyperparameterTuner(estimator = xgb, # The estimator object to use as the basis for the training jobs.\n",
    "                                               objective_metric_name = 'validation:logloss', # The metric used to compare trained models.\n",
    "                                               objective_type = 'Minimize', # Whether we wish to minimize or maximize the metric.\n",
    "                                               max_jobs = 6, # The total number of models to train\n",
    "                                               max_parallel_jobs = 3, # The number of models to train in parallel\n",
    "                                               hyperparameter_ranges = {\n",
    "                                                    'max_depth': IntegerParameter(3, 12),\n",
    "                                                    'eta'      : ContinuousParameter(0.05, 0.5),\n",
    "                                                    'min_child_weight': IntegerParameter(2, 8),\n",
    "                                                    'subsample': ContinuousParameter(0.5, 0.9),\n",
    "                                                    'gamma': ContinuousParameter(0, 10),\n",
    "                                               })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d7e4cd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "#the datasets that were uploaded to s3 to train the model are prepared to use as input for the algorithm\n",
    "\n",
    "s3_input_train = sagemaker.s3_input(s3_data=train_location, content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data=val_location, content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f25850df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The fit method is called on the tuner so Sagemaker can start the training jobs and estimating the models\n",
    "#and their metrics. the train and validation datasets in S3 are passed as parameters\n",
    "xgb_hyperparameter_tuner.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f8e1edc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".................................................................................!\n"
     ]
    }
   ],
   "source": [
    "#Since the training jobs are running on sagemaker but it cannot be visualized, the wait method\n",
    "#is called so it is visible when the training is complete\n",
    "xgb_hyperparameter_tuner.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "826d7bd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-07-01 01:00:55 Starting - Preparing the instances for training\n",
      "2021-07-01 01:00:55 Downloading - Downloading input data\n",
      "2021-07-01 01:00:55 Training - Training image download completed. Training in progress.\n",
      "2021-07-01 01:00:55 Uploading - Uploading generated training model\n",
      "2021-07-01 01:00:55 Completed - Training job completed\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2021-07-01:01:00:44:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2021-07-01:01:00:44:INFO] Setting up HPO optimized metric to be : logloss\u001b[0m\n",
      "\u001b[34m[2021-07-01:01:00:44:INFO] File size need to be processed in the node: 15.59mb. Available memory size in the node: 8419.11mb\u001b[0m\n",
      "\u001b[34m[2021-07-01:01:00:44:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[01:00:44] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[01:00:44] 66453x10 matrix with 664530 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[2021-07-01:01:00:44:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[01:00:44] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[01:00:44] 16613x10 matrix with 166130 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[01:00:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[0]#011train-logloss:0.53616#011validation-logloss:0.53579\u001b[0m\n",
      "\u001b[34mMultiple eval metrics have been passed: 'validation-logloss' will be used for early stopping.\n",
      "\u001b[0m\n",
      "\u001b[34mWill train until validation-logloss hasn't improved in 10 rounds.\u001b[0m\n",
      "\u001b[34m[01:00:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[1]#011train-logloss:0.480268#011validation-logloss:0.480006\u001b[0m\n",
      "\u001b[34m[01:00:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[2]#011train-logloss:0.454563#011validation-logloss:0.454533\u001b[0m\n",
      "\u001b[34m[01:00:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[3]#011train-logloss:0.44143#011validation-logloss:0.441298\u001b[0m\n",
      "\u001b[34m[01:00:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 22 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[4]#011train-logloss:0.434373#011validation-logloss:0.434709\u001b[0m\n",
      "\u001b[34m[01:00:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 30 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[5]#011train-logloss:0.430229#011validation-logloss:0.430858\u001b[0m\n",
      "\u001b[34m[01:00:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 46 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[6]#011train-logloss:0.42811#011validation-logloss:0.428997\u001b[0m\n",
      "\u001b[34m[01:00:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 44 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[7]#011train-logloss:0.426919#011validation-logloss:0.42804\u001b[0m\n",
      "\u001b[34m[01:00:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 38 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[8]#011train-logloss:0.425996#011validation-logloss:0.427046\u001b[0m\n",
      "\u001b[34m[01:00:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 46 pruned nodes, max_depth=1\u001b[0m\n",
      "\u001b[34m[9]#011train-logloss:0.425816#011validation-logloss:0.426831\u001b[0m\n",
      "\u001b[34m[01:00:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 30 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10]#011train-logloss:0.425501#011validation-logloss:0.426529\u001b[0m\n",
      "\u001b[34m[01:00:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 50 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[11]#011train-logloss:0.425114#011validation-logloss:0.426508\u001b[0m\n",
      "\u001b[34m[01:00:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 2 extra nodes, 54 pruned nodes, max_depth=1\u001b[0m\n",
      "\u001b[34m[12]#011train-logloss:0.425079#011validation-logloss:0.426522\u001b[0m\n",
      "\u001b[34m[01:00:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 52 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[13]#011train-logloss:0.425075#011validation-logloss:0.426509\u001b[0m\n",
      "\u001b[34m[01:00:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 48 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[14]#011train-logloss:0.424979#011validation-logloss:0.426358\u001b[0m\n",
      "\u001b[34m[01:00:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 34 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[15]#011train-logloss:0.424839#011validation-logloss:0.426097\u001b[0m\n",
      "\u001b[34m[01:00:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 50 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[16]#011train-logloss:0.424838#011validation-logloss:0.426083\u001b[0m\n",
      "\u001b[34m[01:00:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 56 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[17]#011train-logloss:0.424842#011validation-logloss:0.42607\u001b[0m\n",
      "\u001b[34m[01:00:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 34 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[18]#011train-logloss:0.424838#011validation-logloss:0.426082\u001b[0m\n",
      "\u001b[34m[01:00:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 38 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[19]#011train-logloss:0.424579#011validation-logloss:0.426105\u001b[0m\n",
      "\u001b[34m[01:00:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 8 extra nodes, 28 pruned nodes, max_depth=4\u001b[0m\n",
      "\u001b[34m[20]#011train-logloss:0.424439#011validation-logloss:0.425907\u001b[0m\n",
      "\u001b[34m[01:00:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 60 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[21]#011train-logloss:0.424439#011validation-logloss:0.42591\u001b[0m\n",
      "\u001b[34m[01:00:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 38 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[22]#011train-logloss:0.424439#011validation-logloss:0.425902\u001b[0m\n",
      "\u001b[34m[01:00:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 58 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[23]#011train-logloss:0.424439#011validation-logloss:0.425901\u001b[0m\n",
      "\u001b[34m[01:00:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 6 extra nodes, 42 pruned nodes, max_depth=3\u001b[0m\n",
      "\u001b[34m[24]#011train-logloss:0.424361#011validation-logloss:0.425823\u001b[0m\n",
      "\u001b[34m[01:00:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 44 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[25]#011train-logloss:0.424359#011validation-logloss:0.425834\u001b[0m\n",
      "\u001b[34m[01:00:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 42 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[26]#011train-logloss:0.424361#011validation-logloss:0.425854\u001b[0m\n",
      "\u001b[34m[01:00:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 36 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[27]#011train-logloss:0.424359#011validation-logloss:0.425838\u001b[0m\n",
      "\u001b[34m[01:00:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 4 extra nodes, 36 pruned nodes, max_depth=2\u001b[0m\n",
      "\u001b[34m[28]#011train-logloss:0.424264#011validation-logloss:0.425994\u001b[0m\n",
      "\u001b[34m[01:00:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 58 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[29]#011train-logloss:0.424263#011validation-logloss:0.425992\u001b[0m\n",
      "\u001b[34m[01:00:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 30 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[30]#011train-logloss:0.424266#011validation-logloss:0.426006\u001b[0m\n",
      "\u001b[34m[01:00:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 48 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[31]#011train-logloss:0.424101#011validation-logloss:0.42619\u001b[0m\n",
      "\u001b[34m[01:00:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 34 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[32]#011train-logloss:0.424098#011validation-logloss:0.426174\u001b[0m\n",
      "\u001b[34m[01:00:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 38 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[33]#011train-logloss:0.424099#011validation-logloss:0.426181\u001b[0m\n",
      "\u001b[34m[01:00:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 0 extra nodes, 40 pruned nodes, max_depth=0\u001b[0m\n",
      "\u001b[34m[34]#011train-logloss:0.424099#011validation-logloss:0.426166\u001b[0m\n",
      "\u001b[34mStopping. Best iteration:\u001b[0m\n",
      "\u001b[34m[24]#011train-logloss:0.424361#011validation-logloss:0.425823\n",
      "\u001b[0m\n",
      "Training seconds: 55\n",
      "Billable seconds: 55\n"
     ]
    }
   ],
   "source": [
    "#When the training jobs are completed, the best model is attached to a new estimator object\n",
    "\n",
    "xgb_attached = sagemaker.estimator.Estimator.attach(xgb_hyperparameter_tuner.best_training_job())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8c1b0a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost-210701-0057-001-7f0840d5\n"
     ]
    }
   ],
   "source": [
    "#The job name is printed so all the details of the job can be found from the sagemaker console\n",
    "print(xgb_attached._current_job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e6864d2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "#A transformer object is scpecified calling the transformer method on the newly  created estimator\n",
    "\n",
    "xgb_transformer = xgb_attached.transformer(instance_count = 1, instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f3f49f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the transformer object the predictions are calculated based on the test dataset previously created\n",
    "\n",
    "xgb_transformer.transform(test_location, content_type='text/csv', split_type='Line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "530b86f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............................\u001b[34mArguments: serve\u001b[0m\n",
      "\u001b[34m[2021-07-01 01:18:29 +0000] [1] [INFO] Starting gunicorn 19.9.0\u001b[0m\n",
      "\u001b[34m[2021-07-01 01:18:29 +0000] [1] [INFO] Listening at: http://0.0.0.0:8080 (1)\u001b[0m\n",
      "\u001b[34m[2021-07-01 01:18:29 +0000] [1] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2021-07-01 01:18:29 +0000] [20] [INFO] Booting worker with pid: 20\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/gunicorn/workers/ggevent.py:65: MonkeyPatchWarning: Monkey-patching ssl after ssl has already been imported may lead to errors, including RecursionError on Python 3.6. It may also silently lead to incorrect behaviour on Python 3.7. Please monkey-patch earlier. See https://github.com/gevent/gevent/issues/1016. Modules that had direct imports (NOT patched): ['urllib3.util (/opt/amazon/lib/python3.7/site-packages/urllib3/util/__init__.py)', 'urllib3.util.ssl_ (/opt/amazon/lib/python3.7/site-packages/urllib3/util/ssl_.py)']. \n",
      "  monkey.patch_all(subprocess=True)\u001b[0m\n",
      "\u001b[34m[2021-07-01:01:18:29:INFO] Model loaded successfully for worker : 20\u001b[0m\n",
      "\u001b[34m[2021-07-01 01:18:29 +0000] [21] [INFO] Booting worker with pid: 21\u001b[0m\n",
      "\u001b[34m[2021-07-01 01:18:29 +0000] [22] [INFO] Booting worker with pid: 22\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/gunicorn/workers/ggevent.py:65: MonkeyPatchWarning: Monkey-patching ssl after ssl has already been imported may lead to errors, including RecursionError on Python 3.6. It may also silently lead to incorrect behaviour on Python 3.7. Please monkey-patch earlier. See https://github.com/gevent/gevent/issues/1016. Modules that had direct imports (NOT patched): ['urllib3.util (/opt/amazon/lib/python3.7/site-packages/urllib3/util/__init__.py)', 'urllib3.util.ssl_ (/opt/amazon/lib/python3.7/site-packages/urllib3/util/ssl_.py)']. \n",
      "  monkey.patch_all(subprocess=True)\u001b[0m\n",
      "\u001b[34m[2021-07-01:01:18:29:INFO] Model loaded successfully for worker : 21\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/gunicorn/workers/ggevent.py:65: MonkeyPatchWarning: Monkey-patching ssl after ssl has already been imported may lead to errors, including RecursionError on Python 3.6. It may also silently lead to incorrect behaviour on Python 3.7. Please monkey-patch earlier. See https://github.com/gevent/gevent/issues/1016. Modules that had direct imports (NOT patched): ['urllib3.util (/opt/amazon/lib/python3.7/site-packages/urllib3/util/__init__.py)', 'urllib3.util.ssl_ (/opt/amazon/lib/python3.7/site-packages/urllib3/util/ssl_.py)']. \n",
      "  monkey.patch_all(subprocess=True)\u001b[0m\n",
      "\u001b[34m[2021-07-01:01:18:29:INFO] Model loaded successfully for worker : 22\u001b[0m\n",
      "\u001b[34m[2021-07-01 01:18:29 +0000] [23] [INFO] Booting worker with pid: 23\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/gunicorn/workers/ggevent.py:65: MonkeyPatchWarning: Monkey-patching ssl after ssl has already been imported may lead to errors, including RecursionError on Python 3.6. It may also silently lead to incorrect behaviour on Python 3.7. Please monkey-patch earlier. See https://github.com/gevent/gevent/issues/1016. Modules that had direct imports (NOT patched): ['urllib3.util (/opt/amazon/lib/python3.7/site-packages/urllib3/util/__init__.py)', 'urllib3.util.ssl_ (/opt/amazon/lib/python3.7/site-packages/urllib3/util/ssl_.py)']. \n",
      "  monkey.patch_all(subprocess=True)\u001b[0m\n",
      "\u001b[34m[2021-07-01:01:18:30:INFO] Model loaded successfully for worker : 23\u001b[0m\n",
      "\u001b[34m[2021-07-01:01:18:34:INFO] Sniff delimiter as ','\u001b[0m\n",
      "\u001b[34m[2021-07-01:01:18:34:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[32m2021-07-01T01:18:33.773:[sagemaker logs]: MaxConcurrentTransforms=4, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Since the transform method is running on sagemaker and is not visible, the wait method is called again\n",
    "\n",
    "xgb_transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "618787f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the directory in which it is wanted to save the predictions made by the transformer is saved as a varible\n",
    "\n",
    "data_dir = '../project/processed_data_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6276cfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-us-east-2-730413480526/xgboost-210701-0057-001-7f0840d5-2021-07-01-01-13-44-933/test.csv.out to processed_data_2/test.csv.out\n"
     ]
    }
   ],
   "source": [
    "#Using the string variable referencing the directory to save the predictions, these are copied from the \n",
    "#S3 bucket to the local directory\n",
    "\n",
    "!aws s3 cp --recursive $xgb_transformer.output_path $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6fa62c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some additional imports are needed to read both the actual labels and the predictions saved\n",
    "\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1f6033b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The predictions are loaded using the pandas library. Since the predictions are floats\n",
    "#(values from 01 to 1), they are rounded.\n",
    "\n",
    "predictions = pd.read_csv(os.path.join(data_dir, 'test.csv.out'), header=None)\n",
    "predictions = [round(num) for num in predictions.squeeze().values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ad41c111",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the labels for the test dataset are uploaded using pandas and are transformed into an array so they can be\n",
    "#compared against the predictions\n",
    "\n",
    "test_y = pd.read_csv(\"processed_data_2/test_y.csv\", header=None)\n",
    "test_y = test_y.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8f2de700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.67      0.76      4615\n",
      "           1       0.73      0.92      0.82      4615\n",
      "\n",
      "    accuracy                           0.79      9230\n",
      "   macro avg       0.81      0.79      0.79      9230\n",
      "weighted avg       0.81      0.79      0.79      9230\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The confusion matriz and classification report are imported from the sklearn package\n",
    "#Finally, the results of the model can be analyzed by a classification report. \n",
    "#The analysis for these results are present on the report document related to this notebook. \n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "print(classification_report(test_y,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9eb0e2ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3069, 1546],\n",
       "       [ 356, 4259]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(test_y,predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
